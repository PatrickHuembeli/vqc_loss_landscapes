# AUTOGENERATED! DO NOT EDIT! File to edit: 03_pennylanecirq.ipynb (unless otherwise specified).

__all__ = ['iterate_minibatches', 'rot_layer', 'entangle_layer', 'Reuploading_model', 'Hessian']

# Cell
import numpy as np
import os

import pennylane as qml
from pennylane import expval, var

from matplotlib import pyplot as plt

import numpy as np

from pennylane.utils import _flatten, unflatten
from pennylane.optimize import GradientDescentOptimizer
from tqdm.notebook import tqdm

def iterate_minibatches(inputs, targets, batch_size):
    """
    Function to iterate minibatches of inputs and labels
    """
    for start_idx in range(0, inputs.shape[0] - batch_size + 1, batch_size):
        idxs = slice(start_idx, start_idx + batch_size)
        yield inputs[idxs], targets[idxs]

def rot_layer(params, width=4, layer=4, x=None):
    """adds rotations to each qubit, U(x) followed by U(theta)

    the parameters are structured the following:
    we need a dimension for each layer, for each qubit, 2 for a parameter theta and w multiplied with x
    and 3 for the 3 rotation dimensions of x"""

    for j in range(width):
        #qml.Rot(*x, wires=j) # To make w*x from paper, I get error so we do it with individual rotations
        # Rot(x,y,z) = Rz(z)Ry(y)Rz(x)
        # So let's just do two param rots
        qml.Rot(*x*params[layer][j][0], wires=j)
        # qml.Rot(*params[layer][j][0], wires=j)
        #qml.Rot(*x, wires=j)
        qml.Rot(*params[layer][j][1] , wires=j)

def entangle_layer(width=4, layer=4):
    """adds entangling gates to even / odd wires"""
    even = [[i, i+1] for i in range(0, width, 2)]
    odd = [[i+1, (i+2)%width] for i in range(0, width, 2)]
    for i in range(int(width/2)):
        if layer%2 == 0:
            qml.CZ(wires=even[i])
        if layer%2 == 1:
            qml.CZ(wires=odd[i])

class Reuploading_model(): # 'forest.wavefunction'
    def __init__(self, device='forest.numpy_wavefunction', width=4, layers=4):
        self.width = width
        self.layers = layers
        self.init_params = np.random.uniform(size=(layers, width,2, 3))
        self.dev = qml.device(device, wires=width)


        def circuit(params, x=None, y=None, Z_measure=False):
            """Constructs the circuit from re-uploading paper"""
            herm_matrices = [np.array([[1,0],[0,0]]), np.array([[0,0],[0,1]])] # Define measurement directions
            if Z_measure:
                herm_matrices = [np.array([[1,0],[0,-1]]), np.array([[1,0],[0,-1]])]
            # Since we have two labels |0><0| and |1><1| are good choice.
            idx = int((y + 1)/2)
            for layer in range(layers): #iteration through layers
                rot_layer(params, width=self.width, layer=layer, x=x)
                entangle_layer(width=self.width, layer=layer)
            return qml.expval(qml.Hermitian(herm_matrices[idx], wires=[0]))

        self.qcircuit = qml.QNode(circuit, self.dev)

    def cost(self, params, x=None, y=None, Z_measure=False):
        """Compute prediction for each input in data batch"""
        loss = 0.0
        for i in range(len(x)):
            fidelity = self.qcircuit(params, x=x[i], y=y[i], Z_measure=Z_measure)
            loss += (1 - fidelity)
        return loss/len(x)

    def test(self, params, x=None, y=None):
        """For prediction of model measure in both directions"""
        output_fidelity = []
        for i in range(len(x)):
            fidelities = []
            for j in [0,1]:
                fidelity = self.qcircuit(params, x=x[i], y=j)
                fidelities.append(fidelity)
            output_fidelity.append(fidelities)
        predicted = self.predicted_labels(output_fidelity)
        return predicted, output_fidelity

    def predicted_labels(self, output_fidelity):
        output_labels = [np.argmax(o) for o in output_fidelity]
        return np.array(output_labels)

    def fidelity(self, state1, state2):
        return np.abs(np.dot(np.conj(state1), state2))

    def accuracy_score(self, y_true, y_pred):
        score = y_true == y_pred
        return score.sum() / len(y_true)

    def save_epoch(self, epoch, lr, loss_list, directory, params, test_accuracy=None, H_train=None, Hev_train=None,
                   H_test=None, Hev_test=None,
                   T=None, Tev=None, target_state="0or1_state"):
        if not os.path.exists(directory):
            os.mkdir(directory)
        dictionary = { "width":self.width, "layers": self.layers,
                       "target state": target_state,
                       "params" : params,
                       "Hessian train":H_train, "Hess Eigenvalues train": Hev_train,
                       "Hessian test":H_test, "Hess Eigenvalues test": Hev_test,
                       "metric Tensor":T,
                       "Tens Eigenvalues": Tev,
                       "learning rate": lr, "loss list": loss_list,
                       "test accuracy": test_accuracy}
        np.save(directory+"/reupload_model_W{}_L{}_epoch{}.npy"
                 .format(self.width, self.layers, epoch), dictionary)

def Hessian(params, model, X=None, y=None, Z_measure = False):
        eps = np.pi/2 # This is parameter shift value
        shape = params.shape
        hess_dim = np.array([j for j in shape]).prod()
        params = params.reshape((hess_dim,))
        Hessian_Matr = np.zeros(shape=(hess_dim, hess_dim))
        print("Hessian Dimension: {}, {}".format(hess_dim, hess_dim))


        for Xbatch, ybatch in iterate_minibatches(X, y, batch_size=X.shape[0]):
            measure_normal = model.cost(params.reshape(shape), Xbatch, ybatch, Z_measure=Z_measure)

        gradient = []
        for k in range(hess_dim):
            eps_plus = params.copy()
            eps_plus[k] += eps
            exp_value_plus = model.cost(eps_plus.reshape(shape), Xbatch, ybatch)

            eps_minus = params.copy()
            eps_minus[k] -= eps
            exp_value_minus = model.cost(eps_minus.reshape(shape), Xbatch, ybatch)

            gradient_result = 0.5 * (exp_value_plus - exp_value_minus)
            gradient.append(gradient_result)
        gradient = np.array(gradient)

        progress = tqdm(range(hess_dim))
        for k in progress:
            for l in range(hess_dim):
                if l<=k:
                    eps_pp = params.copy()
                    eps_pp[k] += eps
                    eps_pp[l] += eps

                    eps_pm = params.copy()
                    eps_pm[k] += eps
                    eps_pm[l] -= eps

                    eps_mp = params.copy()
                    eps_mp[k] -= eps
                    eps_mp[l] += eps

                    eps_mm = params.copy()
                    eps_mm[k] -= eps
                    eps_mm[l] -= eps

                    measure_pp = model.cost(eps_pp.reshape(shape), Xbatch, ybatch)
                    measure_pm = model.cost(eps_pm.reshape(shape), Xbatch, ybatch)
                    measure_mp = model.cost(eps_mp.reshape(shape), Xbatch, ybatch)
                    measure_mm = model.cost(eps_mm.reshape(shape), Xbatch, ybatch)
                    #result_eps_pp = torch.tensor([exp_value_pp])
                    #print(measure_mm, measure_mp)
                    hessian_result = 0.25 * (measure_pp + measure_mm - measure_mp - measure_pm)
                    # for loss = (1-f)**2, hessian is 2f'f' + 2(f-1)f''
                    #Hessian_Matr[k][l] = 2*gradient[k]*gradient[l] + 2*(measure_normal - 1)*hessian_result

                    # for loss = (1-f), hessian is -f''
                    Hessian_Matr[k][l] = hessian_result
        for k in range(hess_dim):
            for l in range(hess_dim):
                if l>k:
                    Hessian_Matr[k][l] = Hessian_Matr[l][k]
        return Hessian_Matr
